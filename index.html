<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="FineCLIPER: Multi-modal Fine-grained CLIP for Dynamic Facial Expression Recognition with AdaptERs">
  <meta name="keywords" content="multi-modal, dynamic facial expression recognition, video understanding">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <title>FineCLIPER: Multi-modal Fine-grained CLIP for Dynamic Facial Expression Recognition with AdaptERs</title>
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link href="static/personal.jpg" rel="icon" media="(prefers-color-scheme: light)" />
  <link href="static/personal.jpg" rel="icon" media="(prefers-color-scheme: dark)" />
  <!-- <link rel="icon" href="/home/haodong/FineCLIPER/PersonaGen-Page/static/personal.jpg"> -->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

</head>

<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-3 publication-title">
            FineCLIPER: Multi-modal Fine-grained CLIP <br>
            for Dynamic Facial Expression Recognition with AdaptERs <br>
          ACM MM'24</h1>
<!--           <div class="is-size-5 publication-authors">
            <span class="author-block">MM 2024</span>
          </div> -->
<div class="is-size-5 publication-authors">
  <!-- 第一排 -->
  <div class="author-row">
    <span class="author-block"><a href="https://haroldchen19.github.io/">Haodong Chen</a><sup>😎</sup></span> &nbsp;&nbsp;&nbsp;
    <span class="author-block"><a href="https://github.com/JethroJames">Haojian Huang</a><sup>🥳</sup></span> &nbsp;&nbsp;&nbsp;
    <span class="author-block"><a href="https://scholar.google.com/citations?user=48A9z_UAAAAJ&hl=en">Junhao Dong</a><sup>🤩</sup></span>&nbsp;&nbsp;&nbsp;
    <span class="author-block"><a href="https://github.com/DuNGEOnmassster">Mingzhe Zheng</a><sup>😎</sup></span> &nbsp;&nbsp;&nbsp;
    <span class="author-block"><a href="https://scholar.google.com/citations?hl=en&user=amxDSLoAAAAJ&view_op=list_works&sortby=pubdate">Dian Shao</a><sup>😎🥰</sup></span> &nbsp;&nbsp;&nbsp;
  </div>
  <!-- 第二排
  <div class="author-row">
    <span class="author-block"><a href="https://jaehong31.github.io/">Jaehong Yoon</a></span> &nbsp;&nbsp;&nbsp;
    <span class="author-block"><a href="https://klauscc.github.io/">Feng Cheng</a></span> &nbsp;&nbsp;&nbsp;
    <span class="author-block"><a href="https://www.gedasbertasius.com/">Gedas Bertasius</a></span> &nbsp;&nbsp;&nbsp;
    <span class="author-block"><a href="https://www.cs.unc.edu/~mbansal/">Mohit Bansal</a></span> 
  </div> -->
</div>
<div class="is-size-5 publication-authors">
  <span class="author-block"><sup>🥰</sup>Corresponding Author</span>
</div>
<div class="is-size-5 publication-authors">
  <span class="author-block"><sup>😎</sup>Northwestern Polytechnical University</span>
</div>
<div class="is-size-5 publication-authors">
  <span class="author-block"><sup>🥳</sup>The University of Hong Kong, <sup>🤩</sup>Nanyang Technological University</span>
</div>
          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2407.02157v1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://haroldchen19.github.io/FineCLIPER-Page/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code (Coming soon)</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
            </div>

        </div>      
      </div>
    </div>  
    <section class="hero is-small">
      <div class="body">
        <div class="container">
          <center>
            <img src="./image/new_fig1.png" width="50%">
          </center>
          <div class="content has-text-justified" style="margin: auto; width: 100%;">
            <center>
              <b>Figure 1: Frameworks for Dynamic Facial Expression Recognition</b>.
            </center>
          </div>
        </div>
      </div>
    </section>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Dynamic Facial Expression Recognition (DFER) is crucial for understanding human behavior.
            However, current methods exhibit limited performance mainly due to the scarcity of high-quality data, the insufficient utilization of facial dynamics, and the ambiguity of expression semantics, etc.
            To this end, we propose a novel framework, named Multi-modal Fine-grained CLIP for Dynamic Facial Expression Recognition with AdaptERs (FineCLIPER), incorporating the following novel designs:    
            1) To better distinguish between similar facial expressions, we extend the class labels to textual descriptions from both positive and negative aspects, and obtain supervision by calculating the cross-modal similarity based on the CLIP model;
            2) Our FineCLIPER adopts a hierarchical manner to effectively mine useful cues from DFE videos. Specifically, besides directly embedding video frames as input (low semantic level),
            we propose to extract the face segmentation masks and landmarks based on each frame (middle semantic level) and utilize the Multi-modal Large Language Model (MLLM) to further generate detailed descriptions of facial changes across frames with designed prompts (high semantic level).
            Additionally, we also adopt Parameter-Efficient Fine-Tuning (PEFT) to enable efficient adaptation of large pre-trained models (i.e., CLIP) for this task.
            Our FineCLIPER achieves SOTA performance on the DFEW, FERV39k, and MAFW datasets in both supervised and zero-shot settings with few tunable parameters. Analysis and ablation studies further validate its effectiveness.  </p>
        </div>
      </div>
    </div>

  </div>
</section>

<br>
<br>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
    <h2 class="title is-3">Method</h2>

      <center><img src="./image/framework.png" alt="Teaser" width="100%"></center>

      <div class="content has-text-justified">
        <b>Figure 2: Overview of FineCLIPER</b>. The framework can be divided into three main components: Label Encoder, Multi-Modal Encoders, and Similarity Calculation. The Label Encoder augments labels using PN descriptors, followed by PN adaptors within text encoder; The Multi-Modal Encoders handle hierarchical information mined from low semantic levels to high semantic levels of human face; The Similarity Calculation module further integrates and computes the similarities of the representations obtained earlier via contrastive learning.
      </div>

      <!-- <center><img src="./image/textgen.png" alt="Teaser" width="60%"></center>

      <div class="content has-text-justified">
        <center><b>Figure 3: Fine-grained Text Generation and Refinement</b>.</center>
      </div> -->

    </div>
  </div>
  </div>
</section>

<br>
<br>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
    <h2 class="title is-3">Main Results</h2>

    </div>
  </div>
  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <div class="content has-text-justified">
        <b>Table 1</b>: Comparisons of our FineCLIPER with the state-of-the-art Supervised DFER methods on DFEW, FERV39k, and MAFW. 
        <sup>&ast;</sup>: FineCLIPER with face parsing and landmarks modalities; <sup>&dagger;</sup>: FineCLIPER with fine-grained text modality.
        The best results are highlighted in Bold, and the second-best Underlined.</div>
        <center><img src="./image/Tab1.png" alt="Teaser" width="100%"></center>
        <div class="content has-text-justified">
        <b>Table 2</b>: Comparative analyses of accuracy across various emotion categories: FineCLIPER vs. other approaches on DFEW.</div>
        <center><img src="./image/Tab2.png" alt="Teaser" width="100%"></center>
        <div class="content has-text-justified">
        <b>Table 3</b>: Comparison with state-of-the-art Zero-Shot DFER methods. <sup>&dagger;</sup>: FineCLIPER with fine-grained text modality.</div>
        <center><img src="./image/Tab3.png" alt="Teaser" width="100%"></center>
    <!-- <div class="content has-text-justified">
    We demonstrate the effectiveness of VideoTree by evaluating it on three standard long video question answering (LVQA) datasets: EgoSchema, which focuses on egocentric long-form video-language understanding; NExT-QA, a widely-used video question answering benchmark featuring videos that average 44 seconds in length; IntentQA, an LVQA dataset focused on reasoning about people’s intent in long videos.
    Across these tasks, VideoTree significantly improves accuracy compared to state-of-the-art LLM-based approaches, achieving an absolute accuracy improvement of 7.0% on EgoSchema benchmark, 2.2% on NExT-QA, and 2.7% on IntentQA.
  </div> -->
  </div>
  </div>


  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h2 class="title is-3">Abalation Study</h2>

    <center><img src="./image/ablation_tab.png" alt="Teaser" width="100%"></center>
    <!-- <center><img src="./images/ablation2.png" alt="Teaser" width="100%"></center> -->

    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <div class="content has-text-justified">
          <b>Figure 3</b>: Visualizations of class-wise cosine similarity values between video and text embeddings in DFEW, where the positive value is in green and the negative one is in red.
        </div>
          <center><img src="./image/ablation_neg.png" alt="Teaser" width="60%"></center>
          <div class="content has-text-justified">
          <b>Figure 4</b>: Comparison between our adaptive weighting strategy and fixed weights on the DFEW dataset, where the x-axis represents the weights of video features.
        </div>
          <center><img src="./image/ablation_weight.png" alt="Teaser" width="100%"></center>
          <div class="content has-text-justified">
            We present a detailed analysis of our FineCLIPER framework, providing comprehensive quantitative and qualitative evaluations that further validate the superiority of FineCLIPER.
          </div>
    </div>
    </div>
  </div>
  </div>

  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h2 class="title is-3">Qualitative Analysis</h2>

    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <div class="content has-text-justified">
          <b>Figure 5</b>: Parameter-Performance comparison on the DFEW testing set. The bubble size indicates the model size.
        </div>
          <center><img src="./image/model_eff.png" alt="Teaser" width="60%"></center>
          <div class="content has-text-justified">
          <b>Figure 6</b>: Attention visualizations for DFEW w.r.t. two ground-truth expression labels: 'Happiness' (Top) and 'Surprise' (Bottom).
        </div>
          <center><img src="./image/atten.png" alt="Teaser" width="100%"></center>
          <div class="content has-text-justified">
            <center><b>Figure 7</b>: Examples of the generated text and the refined text.</center>
          </div>
            <center><img src="./image/fine_text.png" alt="Teaser" width="100%"></center>
    </div>
    </div>
  </div>
  </div>


</section>

<br>
<br>

<!-- <section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
    <h2 class="title is-3">Qualitative Analysis</h2>

<center><img src="./images/vis1.png" alt="Teaser" width="100%"></center>
<center><img src="./images/vis2.png" alt="Teaser" width="100%"></center>
      <div class="content has-text-justified">
 <p>
We visualize qualitative results from VideoTree. Specifically, we show the keyframes
  and their captions extracted by our adaptive tree representation given a video query. This example
  is drawn from EgoSchema, and shows the query format, which consists of a query and multiple
  choice answers. With the proposed VideoTree strategy, we can split a complex multi-scene video
  (e.g.cleaning house across rooms) into several key scenes via visual clustering and determine the most
  query-relevant scene via the relevance score. We then can obtain more fine-grained visual cues by
  descending into each relevant cluster (Levels 2 and 3 in Figure 5 top). For example “C opens a washing
  machine” is deemed highly relevant to the question, which asks about the sequence of events. At the
  same time, frames like “C moves around” are deemed irrelevant to the query and not expanded. In
  the end, VideoTree shows a dynamic ability to select relevant segments and can answer the given
  question correctly with only 50% of the baseline’s 32 input captions. The baseline (fixed uniformly
  sampling) fails to correctly answer the question, sampling a large number of redundant and irrelevant
  frames.
</p>
      </div>
    </div>
  </div>
  </div>
</section> -->

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{chen2024finecliper,
  title={FineCLIPER: Multi-modal Fine-grained CLIP for Dynamic Facial Expression Recognition with AdaptERs},
  author={Chen, Haodong and Huang, Haojian and Dong, Junhao and Zheng, Mingzhe and Shao, Dian},
  journal={arXiv preprint arXiv:2407.02157},
  year={2024}
  }</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          This website is constructed using the source code by <a href="https://github.com/nerfies/nerfies.github.io" target="_blank">Nerfies</a>, we are grateful for their contributions.
        </div>
      </div>
    </div>
  </div>
</footer>

  <footer class="container">
    <br><hr>
    <div class="row" style="text-align: center">
      © 2024 Haodong Chen
    </div>
  </footer>
  

</body>
</html>
